# Conclusion {#conclusion}

## Prerequisites {-}

Like the introduction, this concluding chapter contains few code chunks.
But its prerequisites are demanding.
It assumes that you have:

- Read-through and attempted the exercises in all the chapters of Part 1 (Foundations).
- Grasped the diversity of methods that build on these foundations, by following the code and prose in Part 2 (Extensions).
- Considered how you can use geocomputation to solve real-world problems, at work and beyond, after engaging with Part 3 (Applications).

The aim is to consolidate knowledge and skills for geocomputation and inspire future directions of application and development.
<!-- Section \@ref(concepts) reviews the content covered in the previous chapters but at a high level. -->
<!-- Previous chapters focus on the details of packages, functions, and arguments needed for geocomputation with R. -->
<!-- This chapter focuses on concepts that recur throughout the book and how they may be useful. -->
Section \@ref(gaps) discusses gaps in the book's contents and explains why some areas of research were deliberately omitted while others were emphasized.
This discussion leads to the question (which is answered in section \@ref(next)): having read this book, where next?
The final section (\@ref(benefit)) returns to the wider issues raised in Chapter \@ref(intro) and considers how geocomputation can be used for social benefit.

<!-- Section \@ref(next) -->

<!-- ## Concepts for geocomputation {#concepts} -->

## Gaps and overlaps {#gaps}

A characteristic of R is that there are often multiple ways to achieve the same result.
Geocomputation with R is no exception.
The code chunk below illustrates this by using three functions covered in Chapters \@ref(attr) and \@ref(geometric-operations)
<!-- from 3 packages (**sf**, base R's **stats** and tidyverse's **dplyr**) -->
based on the **sf** package to combine the 16 regions of New Zealand into a single geometry:

```{r}
library(spData)
nz_u1 = sf::st_union(nz)
nz_u2 = aggregate(nz["Population"], list(rep(1, nrow(nz))), sum)
nz_u3 = dplyr::summarise(nz, t = sum(Population))
identical(nz_u1, nz_u2$geometry)
identical(nz_u1, nz_u3$geom)
```

Although the classes, attributes and column names of the resulting objects `nz_u1` to `nz_u3` differ, their geometries are identical.
This is verified using the base R function `identical()`.^[
The first operation, undertaken by the function `st_union()`, creates an object of class `sfc` (a simple feature column).
The latter two operations create `sf` objects, each of which *contains* a simple feature column.
Therefore it is the geometries contained in simple feature columns, not the objects themselves, that are identical.
]
Which to use?
It depends.
The former only processes the geometry data contained in `nz` so is faster.
The other options performed attribute operations, which may be useful for subsequent steps.

The wider point is that there are often multiple options to choose from when working with geographic data in R, even within a single package.
The range of options grows further when other R packages are considered: you could acheive the same result using the older **sp** package, for example.
We recommend using **sf** and the other packages showcased in this book, for reasons outlined in Chapter \@ref(spatial-class), but it's worth being aware of alternatives and being able to justify your choice of software.

We deliberately covered both **tidyverse** and base R approaches to attribute data operations.
Chapter \@ref(attr) showed how both `nz[, "Name"]` and `nz %>% select(Name)` can be used to achieve the same result, for example.
The overlap is highlighted because each approach has advantages: the pipe syntax is popular and appealing to some, while base R is more stable, and is well-known to others.
Choosing between them is therefore largely a matter of preference, but beware of pitfalls when using **tidyverse** functions to handle spatial data (see the supplementary article `spatial-tidyverse` at [geocompr.github.io](https://geocompr.github.io/)).

<!-- Not entirely sure, what you are referring to here. Which two packages? Which other functions were omitted? Maybe reference also the corresponding section.-->
While commonly needed subsetting functions were covered in depth, in two packages, many others were omitted. 
Chapter \@ref(intro) mentions 20+ influential spatial packages that have been developed over the years.
Although each package covered in this book has a different emphasis, there are overlaps between them and it is important to remember that there are dozens of packages for geographic data *not* covered in this book.
There are 176 packages in the Spatial [Task View](https://cran.r-project.org/web/views/) alone (as of summer 2018); more packages and countless functions for geographic data are developed each year.
It would be impossible to do justice to all of them in a single book.

```{r, eval=FALSE, echo=FALSE}
# aim: find number of packages in the spatial task view
# how? see:
# vignette("selectorgadget")
stv_pkgs = xml2::read_html("https://cran.r-project.org/web/views/Spatial.html")
pkgs = rvest::html_nodes(stv_pkgs, "ul:nth-child(5) a")
pkgs_char = rvest::html_text(pkgs)
length(pkgs_char)
```

The volume and rate of evolution in R's spatial ecosystem may seem overwhelming.
Our advice in this context applies equally to other domains of knowledge: learn how to use one thing *in depth* but have a general understand of the *breadth* of options available to solve problems in your domain with other R packages (section \@ref(next) covers developments in other languages).
Of course, some packages perform much better than others, making package selection an important decision.
From this diversity, we have focussed on packages that are future-proof (they will work long into the future), high performance (relative to other R packages) and complimentary.
But there is still overlap in the packages we have used, as illustrated by the diversity of packages for making maps, for example (see Chapter \@ref(adv-map)).

Package overlap is not a bad thing.
It increases resilience, drives performance (partly driven by friendly competition and mutual learning) and provides choice in the R-spatial ecosystem.
Being able to choose between multiple options is a key feature of open source software.
When using a particular 'stack', such as the **sf**/**tidyverse**/**raster** ecosystem advocated in this book, it is worth being aware of alternatives already developed (such as **sp**/**rgdal**/**rgeos**) and, where possible, promissing alternatives that are under development (such as **stars**).

Anther consideration is gaps and overlaps in the topics covered in this book.
We have been selective, emphasizing some topics while omitting others.
We have tried to emphasize topics that are most commonly needed in real-world applications such as geographic data operations, projections, data read/write and visualization.
On the other hand, we have omitted topics that are less commonly used, or which are well catered for in other resources.
Point pattern analysis, spatial interpolation (kriging) and spatial epidemiological modeling, for example, are important topics.
But there is already excellent material on these things such as @baddeley_spatial_2015 and @bivand_applied_2013.
<!-- one could also add @brunsdon_introduction_2015 -->
<!-- Todo: add citations to these materials (RL) -->
Another topic which we barely touched is remote sensing though especially everything related to raster analysis is a good introduction to remote sensing with R.
If you want to know more, you might find @wegmann_remote_2016 interesting.

Instead of providing the reader with spatial statistical modeling and inference, we mainly chose to present machine-learning algorithms (see Chapters \@ref(spatial-cv) and \@ref(eco)).
Again, the reason was that there are already great books out there covering these topics, especially with ecological use cases [among others, @zuur_mixed_2009, @zuur_beginners_2017]. 
In case, you are more interested in spatial statistics using Bayesian modeling, check out also @blangiardo_spatial_2015.
<!-- @Robinlovelace, as far as I remember blangiardo also were using epidemiological use cases. Zuur et al. also agreed to write a book on spatial, and spatial-temporal models for medical, public health and epidemiological data analysis using R-INLA -> see highstat.com -->

<!-- maybe, we should put this into the preface as well? -->
Finally, we have largely omitted big data analytics.
This might seem surprising since especially geographic data can become big really fast. 
But the prerequisite for doing big data analytics is to know how to solve a problem on a small dataset.
Once you have learned that you can apply the exact same techniques big data questions, though of course you need to expand your toolbox. 
The first thing to learn is to handle spatial data queries.
This is because big data analytics often boil down to extracting a small amount of data from a database for a specific statistical analysis.
For this, we have provided an introduction to spatial databases and how to use a GIS from within R in chapter \@ref(gis).
If you really have to do the analysis on a big or even the complete dataset, hopefully, the problem you are trying to solve is embarassingly parallel.
For this, you need to learn a system that is able to do this parallelization efficiently such as Hadoop, GeoMesa (http://www.geomesa.org/) or GeoSpark [http://geospark.datasyslab.org/; @huang_geospark_2017].
But still, you are applying the same techniques and concepts you have used on small datasets to answer a big data question, the only difference is that you then do it in a big data setting.

<!-- Likewise, there are gaps and overlaps in the contents of this book, which are worth considering before we consider next steps in section \@ref(next). -->
<!-- More than 15 years ago, before most of the packages used in this book had been developed,  -->

## Where next? {#next}

Learning geocomputation with R is challenging and there is much more to discover.
We have progressed quickly.
The jump from chapters \@ref(spatial-class) to \@ref(eco) is large:
the creation and manipulation of simple spatial objects in Part I may seem a world away from the analysis of large datasets covered in Part III.
It is impossible to become an expert in any area by reading a single book, and skills must be practiced.
This section points toward sensible next steps on your geocomputational journey, **highlighted in bold below**.
<!-- and ordered by difficulty, beginning with continue to improve your knowledge of R. -->

The language of R is a shared thread connecting all the chapters.
All the analyses are based on R classes, primarily `sf` and `raster`, which in turn build on the base R classes of `data.frame` and `matrix`.
The wider point is the importance of *depth of understanding*.
This observation suggests a direction of travel: **improving your understanding of the R language**.
A next step in this direction is to deepen your knowledge of base R, for example by: studying R's key documents (which can be found by entering `help.start()` in the console), reading and playing with the source code of useful functions, or reading comprehensive resources on the subject such those by @wickham_advanced_2014 and @chambers_extending_2016.
<!-- creating and querying simple spatial in Chapter \ref(spatial-class) -->
<!-- maybe we should add info about places to learn more r-spatial stuff (aka github, twitter, ...?)? -->

<!-- Many directions of travel could be taken after taking the geocomputational steps -->
Perhaps the most obvious direction for future learning is **discovering geocomputation with other languages**.
There are good reasons for learning R as a language for geocomputation, as described in Chapter \@ref(intro), but it is not the only option.^[
R's strengths relevant to our definition geocomputation include its emphasis on scientific reproducibility, widespread use in academic research and unparalleled support for statistical modeling of geographic data.
Furthermore, we advocate learning one language (R) for geocomputation in depth before delving into other languages/frameworks because of the costs associated with context switching.
It is preferable to have expertise in one language than basic knowledge of many.
]
It is possible to study *Geocomputation with Python*, *C++*, *JavaScript*, *Scala*, *Julia* or *Rust* in equal depth.
Each of these is a promising language for geocomputation.
[**Turf.js**](https://github.com/Turfjs/turf), for example, provides many functions for geospatial analysis with implementations in JavaScript, Julia, and even Swift.
[**rasterio**](https://github.com/mapbox/rasterio) is a Python package for raster offering a high-performance interface to GDAL for handling raster data.
These and other tantalizing geospatial software projects can be found on the GitHub repo [Awesome-Geospatial](https://github.com/sacridini/Awesome-Geospatial).

<!-- misc ideas: -->
<!-- - learning the geocomputation history (e.g. great papers by S. Openshaw) -->
<!-- - learning about new geocomputation methods (not implemented) -->
<!-- - reading about new non-spatial methods and be inspired (e.g. from fields of image analysis or geometry) -->
<!-- - combining methods from outside R with R -->
<!-- - creating new methods (reference to ch 10) -->

## Geo* for social benefit {#benefit}

This is a technical book so it makes sense for the next steps to also be technical.
But there are many non-technical issues to consider, now you understand what is possible with geographic data in R.
This section returns to the defintion of geocomputation and wider issues covered in Chapter \@ref(intro).
It argues for the methods to be used to tackle some of the planet's most pressing problems.
The use of geo* rather than geocomputation is deliberate.
Many terms, including geographic data science, geographic information systems and geoinformatics, capture the range of possibilities opened-up by geospatial software and knowledge of data.
But geocomputation has advantages: a concise term that defines a field with with three main ingredients:

- The *creative* use of geographic data.
- Application to *real-world problems* for social benefit.
- Building tools using a 'scientific' approach [@openshaw_geocomputation_2000].

Only one of these ingredients is technical.
We believe the broader non-technical aims are what make geospatial work so rewarding, and this is an asset of geocomputation: its application to solve important problems.
What is the point of building a new geographic method (tool) if its only purpose is to increase sales of perfume?
<!-- BOOM! None. -->

<!-- A bit of a rapid jump to reproducibility, I suggest another paragraph goes before this one (RL) -->
Reproducibility is an additional ingredient that can ensure geo* work is socially beneficial, or at least benign.
It supports *creativity*, encouraging the focus of methods to shift away from the basics (which are readily available through shared code, avoiding many people 'reinventing the wheel') and toward applications.
<!-- that nobody has yet thought of. -->
Reproducibility encourages geocomputation for social benefit because it makes geographic data analysis publicly accessible and transparent.

The benefits of reproducibility can be illustrated with the example of using geocomputation to increase sales of perfume.
If the methods are hidden and cannot reproduced, few people can benefit (except perhaps perfume companies!).
If the underlying code is made open and reproducible, by contrast, the methods can be re-purposed.
Reproducibility encourages socially beneficial collaboration.^[
One accessible way to contribute upstream is creating a reprex (reproducible example) to highlight a bug in the package's issue tracker, as outlined in section \@ref(scripts).
]
The importance of reproducibility, and other non-technical ingredients in the field of geocompuation, are further discussed in an open access artical celebrating '21+ years of geocomputation' [@harris_more_2017].

<!-- Like any worthwhile intellectual endeavor or nascent academic field, geocomputation is diverse and contested. -->
